{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0077,
     "end_time": "2025-04-04T13:51:17.531445",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.523745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This Notebook is forked from the 1st Place Winner of ARC Prize 2024\n",
    "\n",
    "## Dark AGI's ARC-2025 Open Source Commitment: All submissions in this competition will be open-sourced to help the community explore AGI capable of achieving 85%+ scores.\n",
    "\n",
    "**Competition Link:** [ARC Prize 2024](https://www.kaggle.com/competitions/arc-prize-2024)  \n",
    "**Original Notebook:** [arc-prize-2024-solution-by-the-architects](https://www.kaggle.com/code/dfranzen/arc-prize-2024-solution-by-the-architects?scriptVersionId=211637468)\n",
    "\n",
    "---\n",
    "\n",
    "## Modifications Implemented\n",
    "\n",
    "1. **4-GPU Support**\n",
    "   - Extended multi-GPU implementation from 2 to 4 GPUs\n",
    "   - Modified dataset splitting logic in `prepare_dataset` to evenly distribute work\n",
    "   - Added training and inference processes for GPU 2 and GPU 3\n",
    "   - Updated subprocess monitoring to wait for all 8 processes (4 training + 4 inference)\n",
    "   - Improved resource utilization and inference throughput by 2x\n",
    "\n",
    "2. **Enhanced Reproducibility**\n",
    "   - Added global seed control (GLOBAL_SEED = 42) with per-GPU deterministic seeding\n",
    "   - Applied consistent seed values to all randomized operations for reproducible results\n",
    "   - Disabled non-deterministic algorithms to ensure consistent outputs across runs\n",
    "   - Implemented seed-based task distribution for consistent GPU workloads\n",
    "\n",
    "3. **Comprehensive Visualization**\n",
    "   - Implemented data visualization for both training and inference phases:\n",
    "     - Color-coded grid displays for ARC tasks with intuitive color mapping\n",
    "     - Side-by-side comparisons of inputs, ground truth, and model predictions\n",
    "   - Added multi-GPU result comparison showing prediction quality across all GPUs\n",
    "   - Created task-specific visualizations showing training examples, test inputs, and prediction attempts\n",
    "   - Calculated detailed accuracy metrics with statistical breakdowns:\n",
    "     - Per-attempt success rates for first and second predictions\n",
    "     - Overall accuracy percentages for both individual attempts\n",
    "     - Combined success rate for either prediction attempt\n",
    "     - Shape and value distribution analysis for predictions vs ground truth\n",
    "     - Non-zero prediction completion rate and zero-prediction filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T13:51:17.54615Z",
     "iopub.status.busy": "2025-04-04T13:51:17.545902Z",
     "iopub.status.idle": "2025-04-04T13:51:17.549486Z",
     "shell.execute_reply": "2025-04-04T13:51:17.548951Z"
    },
    "papermill": {
     "duration": 0.012497,
     "end_time": "2025-04-04T13:51:17.550971",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.538474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Daniel Franzen and Jan Disselhoff\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T13:51:17.565451Z",
     "iopub.status.busy": "2025-04-04T13:51:17.565051Z",
     "iopub.status.idle": "2025-04-04T13:51:17.567719Z",
     "shell.execute_reply": "2025-04-04T13:51:17.567193Z"
    },
    "papermill": {
     "duration": 0.011385,
     "end_time": "2025-04-04T13:51:17.56911",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.557725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This notebook contains our winning submission to the ARC Prize 2024 Kaggle competition,\n",
    "# scoring 53.5 points on the private evaluation set.\n",
    "# the ARChitects (Daniel Franzen and Jan Disselhoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00656,
     "end_time": "2025-04-04T13:51:17.582367",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.575807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Runner Module Explanation\n",
    "\n",
    "This Python module (`model_runner.py`) is a comprehensive toolkit for working with language models, focusing on efficiency and performance optimization. The code is designed to handle various aspects of model management including loading, training, inference, and optimization.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### Tokenizer Optimization\n",
    "The module provides several functions for optimizing tokenizers:\n",
    "- `indices_required_for_merges` identifies necessary token IDs for BPE merges\n",
    "- `remove_unused_merges` cleans up unused merge rules\n",
    "- `shrink_tokenizer_vocab` reduces vocabulary size while maintaining model functionality\n",
    "- `remove_tokenizer_normalizer` removes normalizer components when not needed\n",
    "\n",
    "### Model Size Reduction\n",
    "The code includes specialized functionality for reducing model size:\n",
    "- `shrink_model_embeddings` resizes embedding tables to match reduced vocabularies\n",
    "- `shrink_embeddings` orchestrates the entire embedding reduction process\n",
    "- Support for 4-bit quantization through `transformers_4bit` and `unsloth_4bit` loading modes\n",
    "\n",
    "### Model Management\n",
    "Several utilities handle model loading and manipulation:\n",
    "- `prepare_model` provides a unified interface for model loading with various options\n",
    "- `merge_peft_into_base` merges fine-tuned adapters into base models\n",
    "- `fix_dtypes` ensures consistent data types across model components\n",
    "- `save_model` handles model saving with options for merging adapters\n",
    "\n",
    "### Training Functions\n",
    "The module supports efficient fine-tuning:\n",
    "- `training_run` handles the training process with support for both standard and optimized trainers\n",
    "- The `Retrainer` class enables retraining with data augmentation\n",
    "- Support for gradient accumulation fixes and packing optimizations\n",
    "\n",
    "### Inference\n",
    "Comprehensive inference capabilities:\n",
    "- `inference_run_v2` orchestrates inference runs across datasets\n",
    "- `inference_turbo_dfs` implements a depth-first search approach for higher quality outputs\n",
    "- `inference_step` handles token generation with various decoding strategies\n",
    "\n",
    "### Result Processing\n",
    "The `Decoder` class provides extensive functionality:\n",
    "- Tracks and evaluates generated outputs against reference solutions\n",
    "- Calculates accuracy metrics based on exact matches\n",
    "- Supports probability tracking for solution ranking\n",
    "- Benchmarks different selection algorithms\n",
    "\n",
    "### Utilities\n",
    "Helpful utilities include:\n",
    "- Compressed storage for inference results via `inference_save`/`inference_load` \n",
    "- PEFT weight management functions\n",
    "- GPU memory tracking via `mem_info`\n",
    "\n",
    "This module appears to be built for competitive or research applications where optimizing model efficiency and generation quality is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.033305,
     "end_time": "2025-04-04T13:51:17.622433",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.589128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_runner.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile model_runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006615,
     "end_time": "2025-04-04T13:51:17.635906",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.629291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ARC Dataset Processing and Formatting Library\n",
    "\n",
    "This code defines a comprehensive library for working with the Abstraction and Reasoning Corpus (ARC) dataset, providing utilities for data loading, manipulation, augmentation, and formatting for machine learning models. Here's an explanation of the main components:\n",
    "\n",
    "## Core Functionality\n",
    "\n",
    "1. **Data Manipulation Utilities**\n",
    "   - `cut_at_token`: Truncates arrays at specified token positions\n",
    "   - `shuffled`: Randomizes array elements with NumPy's permutation\n",
    "   - `permute_mod`: Applies number permutations to arrays with inversion control\n",
    "   - Various permutation strategies (random, frequency-based) for data augmentation\n",
    "\n",
    "2. **ArcDataset Class**\n",
    "   - Handles loading and processing of ARC challenge datasets\n",
    "   - Provides comprehensive dataset manipulation methods:\n",
    "     - Data augmentation (rotation, transposition, permutation)\n",
    "     - Task filtering and sorting\n",
    "     - Example shuffling and selection\n",
    "     - Dataset splitting and concatenation\n",
    "   - Includes utilities for submission creation and validation\n",
    "\n",
    "3. **ArcFormatter Class**\n",
    "   - Converts grid-based ARC tasks into text format for language models\n",
    "   - Configurable formatting with options for prefixes, separators, and tokenization\n",
    "   - Handles decoding model outputs back into valid grid solutions\n",
    "   - Supports scoring and evaluation of predictions\n",
    "   - Includes methods for formatting train-test examples and queries\n",
    "\n",
    "4. **Custom Data Collator**\n",
    "   - Implements special handling for training language models on ARC tasks\n",
    "   - Supports advanced techniques like output masking and controlled fault injection\n",
    "   - Configurable through options like `fault_freq` and `mask_first_output`\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Data Augmentation**: Extensive options for task transformation to increase training data variety\n",
    "- **Formatting Flexibility**: Customizable text representations for different model preferences\n",
    "- **Length Management**: Methods to filter and truncate tasks to fit model context windows\n",
    "- **Submission Handling**: Tools for generating and validating competition submissions\n",
    "- **Predefined Formatters**: Ready-to-use configurations like `ArcFormatter_pretext2` with different masking strategies\n",
    "\n",
    "This library provides the infrastructure needed to process ARC tasks for machine learning models, handling the conversion between grid-based puzzle representations and the text formats needed by language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.027922,
     "end_time": "2025-04-04T13:51:17.670792",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.64287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing arc_loader.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile arc_loader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00668,
     "end_time": "2025-04-04T13:51:17.684278",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.677598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Selection Algorithms for ARC Competition Solutions\n",
    "\n",
    "The code defines a collection of selection algorithms designed to choose optimal solutions from multiple model predictions for the Abstraction and Reasoning Corpus (ARC) competition. This is a critical component in the submission pipeline, as it determines which predictions will be submitted as final answers.\n",
    "\n",
    "At its core, the selection module offers several strategies for filtering and ranking candidate solutions based on different criteria. The simplest approach is `first_only`, which simply takes the first prediction, reflecting a high confidence in the model's initial guess. The `keep_order` algorithm preserves all predictions in their original sequence, useful when the ordering already reflects confidence levels. For eliminating redundancy, `keep_order_unique` builds on this by removing duplicate solutions.\n",
    "\n",
    "The more sophisticated selection strategies leverage scoring mechanisms. `get_best_shape_by_score` groups predictions by their output shape (dimensions) and identifies the most promising shape based on a scoring function. This is particularly valuable in ARC problems where correct solutions often share consistent dimensions. The `score_sum` function extends this concept by accumulating scores for unique outputs while optionally preferring answers that match the most common output shape.\n",
    "\n",
    "Two notable scoring implementations are provided: `score_all_probsum`, which converts log probabilities to probabilities and sums them to rank solutions, and `score_full_probmul_3`, which incorporates both inference scores and augmented scores with a baseline offset of 3. This combined approach aims to balance the model's direct confidence (inference scores) with additional evaluation metrics (augmented scores) for more robust selection.\n",
    "\n",
    "The code includes utility functions like `hashable` and `make_unique` to handle the array-based outputs, ensuring proper comparison and deduplication. All these algorithms are collected in `selection_algorithms`, allowing for benchmarking different strategies against each other to determine the optimal approach for the final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.013721,
     "end_time": "2025-04-04T13:51:17.704877",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.691156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing selection.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile selection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006815,
     "end_time": "2025-04-04T13:51:17.718539",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.711724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Asynchronous Subprocess Handling with Streaming Output\n",
    "\n",
    "This code provides a set of asynchronous utilities for executing and monitoring subprocesses in Python. Built on top of Python's `asyncio` library, these functions enable efficient parallel execution of external processes while capturing their output streams in real-time.\n",
    "\n",
    "The `stream_reader` function serves as the core component, continuously reading from a subprocess's output stream (either stdout or stderr) in manageable chunks of 4KB. It implements a clever buffering mechanism to ensure complete lines are processed properly. By appending a sentinel character ('X') and using Python's unpacking syntax, it elegantly separates complete lines from partial data that might be cut off mid-line. Each complete line is optionally prefixed with an identifier and directed to the specified output stream.\n",
    "\n",
    "The `wait_for_subprocess` function builds on this foundation by simultaneously monitoring both the stdout and stderr streams of a single subprocess. It uses `asyncio.gather` to concurrently process both streams until completion, then waits for the subprocess to terminate and returns its exit code. The `print_output` parameter provides control over whether the subprocess output should be displayed, while the `id` parameter helps distinguish between outputs from different processes when multiple are running.\n",
    "\n",
    "Finally, `wait_for_subprocesses` extends this capability to handle multiple subprocesses concurrently. It automatically assigns sequential numeric identifiers to each process when more than one is being monitored, making it easier to distinguish their outputs in a multiplexed console display.\n",
    "\n",
    "This asynchronous approach is particularly valuable in data processing pipelines that might involve multiple external tools or long-running computations. Rather than blocking while waiting for each process to complete sequentially, these functions allow the Python program to efficiently manage multiple concurrent tasks, potentially improving overall throughput while maintaining organized output capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.012944,
     "end_time": "2025-04-04T13:51:17.738367",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.725423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing async_tools.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile async_tools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006868,
     "end_time": "2025-04-04T13:51:17.752175",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.745307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comprehensive Framework for ARC Challenge Solution Pipeline\n",
    "\n",
    "This code defines a robust configuration and execution framework for tackling the Abstraction and Reasoning Corpus (ARC) challenge. It serves as the central orchestration module for the entire solution pipeline, connecting data loading, model training, inference, and result submission generation.\n",
    "\n",
    "The file begins by establishing essential paths and configuration settings, including locations for the ARC challenge data and temporary storage directories for model weights and inference outputs. It then loads the test dataset and conditionally loads solution data if working with a \"fake\" test set (likely for validation purposes). The configuration specifies the use of a pre-trained NeMo Mini model with the specialized ArcFormatter_premix_3 for data processing.\n",
    "\n",
    "At the heart of the implementation are two key preparation functions: `prepare_run` and `prepare_dataset`. The `prepare_run` function configures the model environment, including GPU assignment and model initialization with LoRA (Low-Rank Adaptation) fine-tuning parameters. It uses Unsloth's 4-bit quantization for efficient memory usage, applies parameter-efficient fine-tuning to various model components with carefully chosen hyperparameters (including rank-stabilized LoRA), and optimizes for long contexts with gradient checkpointing.\n",
    "\n",
    "The `prepare_dataset` function handles dataset preparation with sophisticated pre-processing techniques. For multi-GPU training, it supports both random splitting and length-based distribution of tasks. The function applies different augmentation strategies depending on whether the dataset is being prepared for training or inference. Training data undergoes rotation, transformation, permutation, and sequence shuffling, while ensuring proper length constraints. Inference data is sorted by input length, augmented, and interleaved to optimize processing.\n",
    "\n",
    "The pipeline continues with two execution functions: `start_training` and `start_inference`. The training function fine-tunes the model using a parameter-efficient approach with 8-bit optimization, cosine learning rate scheduling, and carefully tuned hyperparameters. The inference function applies the trained model to generate solutions, with optional task-specific fine-tuning through the Retrainer class and augmented scoring to enhance solution quality.\n",
    "\n",
    "A notable protective mechanism is the `RemapCudaOOM` context manager, which gracefully handles CUDA out-of-memory errors by creating a placeholder submission file rather than failing catastrophically. This ensures that even under resource constraints, the system can produce a valid competition entry.\n",
    "\n",
    "This comprehensive framework represents a sophisticated approach to the ARC challenge, incorporating advanced techniques in efficient model fine-tuning, data augmentation, and robust error handling to maximize performance within Kaggle's computational constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.017248,
     "end_time": "2025-04-04T13:51:17.776316",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.759068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing common_stuff.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile common_stuff.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007161,
     "end_time": "2025-04-04T13:51:17.790477",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.783316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Environment Setup for ARC Challenge Pipeline\n",
    "\n",
    "This code snippet represents the initialization phase of the ARC challenge solution pipeline. It performs critical setup tasks before the main training and inference processes begin. Let me walk through what's happening:\n",
    "\n",
    "## Environment Configuration and Unsloth Installation\n",
    "\n",
    "The code begins by importing all components from the `common_stuff` module, which contains the core functionality for the ARC solution pipeline as seen in previous sections. It then disables Weights & Biases logging by setting the `WANDB_DISABLED` environment variable to `\"true\"`.\n",
    "\n",
    "A key component is the custom installation and patching of the Unsloth library. Unsloth provides efficient optimization techniques for large language models, but requires specific modifications to work properly in this environment:\n",
    "\n",
    "1. The code checks if Unsloth is already installed by looking for a marker file\n",
    "2. If not installed, it:\n",
    "   - Uninstalls existing PyTorch and Accelerate packages to avoid conflicts\n",
    "   - Installs Unsloth from a local wheel file (avoiding internet downloads, which is important in Kaggle environments)\n",
    "   - Applies several patches to the Unsloth source code:\n",
    "     - Disables the `get_statistics()` function to fix a delay bug\n",
    "     - Removes multi-GPU detection restrictions to enable distributed training\n",
    "   - Creates a marker file to indicate successful installation\n",
    "\n",
    "This custom installation approach ensures compatibility with the Kaggle environment while enabling optimized training performance.\n",
    "\n",
    "## Training Preparation and Cleanup\n",
    "\n",
    "After setting up the environment, the code prepares for training by:\n",
    "\n",
    "1. Removing any \"done\" signal files from previous runs for both GPUs (0 and 1)\n",
    "   - This ensures that the training and inference processes won't mistakenly think a previous run completed successfully\n",
    "\n",
    "2. For debugging scenarios (when using a \"fake\" test set), there are commented-out commands to remove previous model outputs and temporary files\n",
    "   - These cleanup commands are disabled (commented out) but could be enabled for debugging purposes\n",
    "\n",
    "This initialization routine establishes a clean, optimized environment for the subsequent model training and inference phases of the ARC challenge solution pipeline. The modified Unsloth library will enable efficient fine-tuning of the large language model with quantization and other optimizations tailored to this specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 115.948666,
     "end_time": "2025-04-04T13:53:13.746093",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.797427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from common_stuff import *\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 115.948666,
     "end_time": "2025-04-04T13:53:13.746093",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.797427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torch as it is not installed.\n",
      "WARNING: Skipping accelerate as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: ./input/unsloth-2024-9-post4/wheelhouse\n",
      "Processing c:\\fun\\ml\\arc-2025\\input\\unsloth-2024-9-post4\\wheelhouse\\unsloth-2024.9.post4-py3-none-any.whl\n",
      "INFO: pip is looking at multiple versions of unsloth to determine which version is compatible with other requirements. This could take a while.\n",
      "Unsloth installed & patched.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch>=2.4.0 (from unsloth) (from versions: none)\n",
      "ERROR: No matching distribution found for torch>=2.4.0\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(tmp_dir, 'unsloth_installed')):  # unsloth offline install - https://stackoverflow.com/a/51646354\n",
    "    !pip uninstall --yes torch accelerate\n",
    "    !pip install --no-index --find-links=./input/unsloth-2024-9-post4/wheelhouse unsloth\n",
    "    #!pip uninstall --yes accelerate fastai torch torchaudio transformers\n",
    "    #!pip install --no-index --find-links=/kaggle/input/unsloth-2024-10-7/wheelhouse unsloth  # do not use grad_acc_fix - trains very slow\n",
    "    #!sed -i 's/if ((post_check - pre_check) >= 1).sum() > 1:/if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\n",
    "    # fix delay bug in get_statistics()\n",
    "    # !sed -i 's/^def get_statistics():/def get_statistics():\\n if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/_utils.py\n",
    "    # fix faulty unsloth multi-gpu detection\n",
    "    # !sed -i \"s/raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')/pass/g\" /opt/conda/lib/python3.10/site-packages/unsloth/tokenizer_utils.py /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py /opt/conda/lib/python3.10/site-packages/unsloth/models/vision.py\n",
    "    os.makedirs(os.path.join(tmp_dir, 'unsloth_installed'), exist_ok=True)\n",
    "    print('Unsloth installed & patched.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 115.948666,
     "end_time": "2025-04-04T13:53:13.746093",
     "exception": false,
     "start_time": "2025-04-04T13:51:17.797427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcommon_stuff\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mWANDB_DISABLED\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\fun\\ml\\ARC-2025\\common_stuff.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 设置全局随机种子\u001b[39;00m\n\u001b[32m     12\u001b[39m GLOBAL_SEED = \u001b[32m42\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "for gpu in [0, 1]: \n",
    "    signal_path = f'{model_temp_storage}_gpu{gpu}_done'\n",
    "    if os.path.exists(signal_path): os.rmdir(signal_path)\n",
    "\n",
    "if arc_test_set.is_fake:  # cleanup? (for debugging)\n",
    "    #!rm -R /kaggle/temp/finetuned_model*\n",
    "    #!rm -R /kaggle/temp/inference_outputs\n",
    "    #!rm -R /kaggle/temp/inference_scoring\n",
    "    #!ls /kaggle/temp\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009428,
     "end_time": "2025-04-04T13:53:13.765013",
     "exception": false,
     "start_time": "2025-04-04T13:53:13.755585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Asynchronous Training Process Initialization\n",
    "\n",
    "This code cell initiates a background training process for the ARC challenge solution pipeline. Let me explain the critical aspects of what's happening here:\n",
    "\n",
    "The cell uses a special Jupyter cell magic `%%python --bg --proc train_proc0` which instructs the notebook to run the Python code in a separate background process rather than within the main notebook execution thread. The `--proc train_proc0` parameter assigns a specific name to this process, making it identifiable for monitoring and management purposes.\n",
    "\n",
    "Within this background process, the code imports all components from the `common_stuff` module, which contains the comprehensive framework for model configuration, dataset preparation, and training execution that we examined earlier. This module provides access to the model architecture, training parameters, data augmentation strategies, and other essential components of the solution pipeline.\n",
    "\n",
    "The core action is the call to `start_training(gpu=0)`, which initiates the model training process on GPU 0. As detailed in the `common_stuff` module, this function will:\n",
    "\n",
    "1. Establish a unique storage path for this GPU's model weights\n",
    "2. Configure the base model with LoRA fine-tuning parameters\n",
    "3. Prepare the dataset with appropriate augmentations for training\n",
    "4. Execute the training process with carefully tuned hyperparameters\n",
    "5. Create a signal file upon completion to indicate that training has finished\n",
    "\n",
    "Running this process in the background allows the notebook to remain responsive while the computationally intensive training occurs. This approach is particularly valuable in a multi-GPU setup, as it enables the initiation of parallel training processes across available GPUs, potentially accelerating the overall solution development. The named process also facilitates monitoring or termination if needed during the potentially lengthy training process.\n",
    "\n",
    "This background execution model is a sophisticated approach to managing computational resources in Jupyter environments, especially for the resource-intensive tasks involved in state-of-the-art AI competition solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 5.286141,
     "end_time": "2025-04-04T13:53:19.060268",
     "exception": false,
     "start_time": "2025-04-04T13:53:13.774127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "GPU 0 Training Data Samples\n",
      "========================================\n",
      "GPU 0 assigned 60 training tasks\n",
      "\n",
      "Task: 12eac192\n",
      "Training Input (first example):\n",
      "[[1 7 7 1 0 8 0 5]\n",
      " [1 7 7 1 1 0 1 0]\n",
      " [8 8 0 0 7 7 7 7]\n",
      " [0 1 0 0 0 0 1 1]\n",
      " [5 0 8 0 1 0 1 1]]\n",
      "\n",
      "Training Output (first example):\n",
      "[[3 7 7 1 0 3 0 3]\n",
      " [3 7 7 1 1 0 3 0]\n",
      " [3 3 0 0 7 7 7 7]\n",
      " [0 3 0 0 0 0 1 1]\n",
      " [3 0 3 0 3 0 1 1]]\n",
      "----------------------------------------\n",
      "\n",
      "========================================\n",
      "GPU 1 Training Data Samples\n",
      "========================================\n",
      "GPU 1 assigned 60 training tasks\n",
      "\n",
      "Task: 281123b4\n",
      "Training Input (first example):\n",
      "[[0 0 8 8 3 5 0 0 5 3 9 0 0 9 3 4 0 0 4]\n",
      " [0 8 8 0 3 5 5 0 5 3 9 9 0 9 3 0 0 4 4]\n",
      " [8 8 8 0 3 0 5 5 0 3 9 9 0 0 3 4 0 0 0]\n",
      " [8 8 0 0 3 0 0 0 0 3 0 0 0 0 3 4 4 4 0]]\n",
      "\n",
      "Training Output (first example):\n",
      "[[9 0 8 9]\n",
      " [9 9 4 9]\n",
      " [9 9 8 0]\n",
      " [4 4 4 0]]\n",
      "----------------------------------------\n",
      "\n",
      "========================================\n",
      "GPU 2 Training Data Samples\n",
      "========================================\n",
      "GPU 2 assigned 60 training tasks\n",
      "\n",
      "Task: 13713586\n",
      "Training Input (first example):\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 7 0 0 0 3 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]]\n",
      "\n",
      "Training Output (first example):\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 5]\n",
      " [0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 5]\n",
      " [0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 5]\n",
      " [0 0 7 7 7 7 3 3 3 3 3 3 3 3 3 3 5]\n",
      " [0 0 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5]\n",
      " [0 0 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5]\n",
      " [0 0 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5]]\n",
      "----------------------------------------\n",
      "\n",
      "========================================\n",
      "GPU 3 Training Data Samples\n",
      "========================================\n",
      "GPU 3 assigned 60 training tasks\n",
      "\n",
      "Task: 09c534e7\n",
      "Training Input (first example):\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
      " [0 0 1 3 1 1 0 0 0 0 0 0 0 0 1 1 4 1 0 0]\n",
      " [0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
      " [0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0]\n",
      " [0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0]\n",
      " [0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 2 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "Training Output (first example):\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
      " [0 0 1 3 3 1 0 0 0 0 0 0 0 0 1 4 4 1 0 0]\n",
      " [0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 4 4 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
      " [0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 3 3 3 3 1 0 1 1 1 0 0 0 1 1 1 0 0]\n",
      " [0 0 1 3 3 3 3 1 1 1 3 1 0 0 0 1 4 1 0 0]\n",
      " [0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 2 2 1 1 1 1 2 1 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 1 2 2 1 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [1 2 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 2 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 2 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simplified ARC data visualization script (English version)\n",
    "from arc_loader import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create ARC color map\n",
    "cmap = colors.ListedColormap(\n",
    "    ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
    "     '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "norm = colors.Normalize(vmin=0, vmax=9)\n",
    "\n",
    "# Load data directly from file\n",
    "arc_challenge_file = './input/arc-prize-2025/arc-agi_test_challenges.json'\n",
    "\n",
    "# Load original data\n",
    "with open(arc_challenge_file, 'r') as f:\n",
    "    arc_data = json.load(f)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "def visualize_arc_example(train_data, test_data, task_id):\n",
    "    \"\"\"Visualize training and test data for an ARC task\"\"\"\n",
    "    # Get number of training and test examples\n",
    "    n_train = len(train_data)\n",
    "    n_test = len(test_data)\n",
    "    \n",
    "    # Create figure large enough for all examples\n",
    "    fig, axes = plt.subplots(2, max(n_train, n_test), figsize=(4*max(n_train, n_test), 8))\n",
    "    fig.suptitle(f\"Task ID: {task_id}\", fontsize=16)\n",
    "    \n",
    "    # Visualize training data\n",
    "    for i in range(n_train):\n",
    "        # Input\n",
    "        axes[0, i].imshow(train_data[i]['input'], cmap=cmap, norm=norm)\n",
    "        axes[0, i].grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "        axes[0, i].set_title(f\"Training #{i+1} - Input\")\n",
    "        axes[0, i].set_xticks([])\n",
    "        axes[0, i].set_yticks([])\n",
    "        \n",
    "        # Output\n",
    "        axes[1, i].imshow(train_data[i]['output'], cmap=cmap, norm=norm)\n",
    "        axes[1, i].grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "        axes[1, i].set_title(f\"Training #{i+1} - Output\")\n",
    "        axes[1, i].set_xticks([])\n",
    "        axes[1, i].set_yticks([])\n",
    "    \n",
    "    # Handle test data visualization\n",
    "    for i in range(n_test):\n",
    "        if i < n_train:\n",
    "            # Already have training data in this column\n",
    "            pass\n",
    "        else:\n",
    "            # Hide unused training cells\n",
    "            if i >= n_train:\n",
    "                axes[0, i].axis('off')\n",
    "                axes[1, i].axis('off')\n",
    "    \n",
    "    # Show first test input\n",
    "    if n_test > 0:\n",
    "        # Create separate figure for test input\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(test_data[0]['input'], cmap=cmap, norm=norm)\n",
    "        plt.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "        plt.title(f\"Test Input - {task_id}\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Simulate 4 GPU data splitting\n",
    "task_ids = list(arc_data.keys())\n",
    "random.shuffle(task_ids)  # Shuffle task order\n",
    "\n",
    "# Assign tasks to each GPU\n",
    "gpu_tasks = {}\n",
    "for gpu_id in range(4):\n",
    "    # Simple equal division - each GPU gets 1/4 of tasks\n",
    "    start_idx = gpu_id * len(task_ids) // 4\n",
    "    end_idx = (gpu_id + 1) * len(task_ids) // 4\n",
    "    gpu_tasks[gpu_id] = task_ids[start_idx:end_idx]\n",
    "\n",
    "# Display training data samples for each GPU\n",
    "for gpu_id in range(4):\n",
    "    assigned_tasks = gpu_tasks[gpu_id]\n",
    "    print(f\"\\n{'='*40}\\nGPU {gpu_id} Training Data Samples\\n{'='*40}\")\n",
    "    print(f\"GPU {gpu_id} assigned {len(assigned_tasks)} training tasks\")\n",
    "    \n",
    "    # Show only first 3 examples\n",
    "    samples = assigned_tasks[:1]\n",
    "    \n",
    "    for task_id in samples:\n",
    "        print(f\"\\nTask: {task_id}\")\n",
    "        \n",
    "        # Get training and test data for this task\n",
    "        train_data = arc_data[task_id]['train']\n",
    "        test_data = arc_data[task_id]['test']\n",
    "        \n",
    "        # Visualize\n",
    "        # visualize_arc_example(train_data, test_data, task_id)\n",
    "        \n",
    "        # Print data matrices\n",
    "        print(\"Training Input (first example):\")\n",
    "        print(np.array(train_data[0]['input']))\n",
    "        print(\"\\nTraining Output (first example):\")\n",
    "        print(np.array(train_data[0]['output']))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.042624,
     "end_time": "2025-04-04T13:53:19.121504",
     "exception": false,
     "start_time": "2025-04-04T13:53:19.07888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%python --bg --proc train_proc0\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcommon_stuff\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m start_training(gpu=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\fun\\ml\\ARC-2025\\common_stuff.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 设置全局随机种子\u001b[39;00m\n\u001b[32m     12\u001b[39m GLOBAL_SEED = \u001b[32m42\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# %%python --bg --proc train_proc0\n",
    "from common_stuff import *\n",
    "start_training(gpu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T13:53:19.158122Z",
     "iopub.status.busy": "2025-04-04T13:53:19.157863Z",
     "iopub.status.idle": "2025-04-04T13:53:19.162759Z",
     "shell.execute_reply": "2025-04-04T13:53:19.162208Z"
    },
    "papermill": {
     "duration": 0.024793,
     "end_time": "2025-04-04T13:53:19.164183",
     "exception": false,
     "start_time": "2025-04-04T13:53:19.13939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc train_proc1\n",
    "from common_stuff import *\n",
    "start_training(gpu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T13:53:19.201234Z",
     "iopub.status.busy": "2025-04-04T13:53:19.200642Z",
     "iopub.status.idle": "2025-04-04T13:53:19.206623Z",
     "shell.execute_reply": "2025-04-04T13:53:19.205445Z"
    },
    "papermill": {
     "duration": 0.02736,
     "end_time": "2025-04-04T13:53:19.209443",
     "exception": false,
     "start_time": "2025-04-04T13:53:19.182083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc train_proc2\n",
    "from common_stuff import *\n",
    "start_training(gpu=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T13:53:19.32093Z",
     "iopub.status.busy": "2025-04-04T13:53:19.320334Z",
     "iopub.status.idle": "2025-04-04T13:53:19.343524Z",
     "shell.execute_reply": "2025-04-04T13:53:19.342374Z"
    },
    "papermill": {
     "duration": 0.097596,
     "end_time": "2025-04-04T13:53:19.347782",
     "exception": false,
     "start_time": "2025-04-04T13:53:19.250186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc train_proc3\n",
    "from common_stuff import *\n",
    "start_training(gpu=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T13:53:19.406042Z",
     "iopub.status.busy": "2025-04-04T13:53:19.405477Z",
     "iopub.status.idle": "2025-04-04T13:53:19.415586Z",
     "shell.execute_reply": "2025-04-04T13:53:19.41447Z"
    },
    "papermill": {
     "duration": 0.032516,
     "end_time": "2025-04-04T13:53:19.418262",
     "exception": false,
     "start_time": "2025-04-04T13:53:19.385746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc0\n",
    "from common_stuff import *\n",
    "start_inference(gpu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T13:53:19.551717Z",
     "iopub.status.busy": "2025-04-04T13:53:19.551247Z",
     "iopub.status.idle": "2025-04-04T13:53:19.558168Z",
     "shell.execute_reply": "2025-04-04T13:53:19.557239Z"
    },
    "papermill": {
     "duration": 0.070388,
     "end_time": "2025-04-04T13:53:19.559861",
     "exception": false,
     "start_time": "2025-04-04T13:53:19.489473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc1\n",
    "from common_stuff import *\n",
    "start_inference(gpu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T13:53:19.601727Z",
     "iopub.status.busy": "2025-04-04T13:53:19.601279Z",
     "iopub.status.idle": "2025-04-04T13:53:19.619819Z",
     "shell.execute_reply": "2025-04-04T13:53:19.618106Z"
    },
    "papermill": {
     "duration": 0.0484,
     "end_time": "2025-04-04T13:53:19.626974",
     "exception": false,
     "start_time": "2025-04-04T13:53:19.578574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc2\n",
    "from common_stuff import *\n",
    "start_inference(gpu=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T13:53:19.731884Z",
     "iopub.status.busy": "2025-04-04T13:53:19.731335Z",
     "iopub.status.idle": "2025-04-04T13:53:19.749241Z",
     "shell.execute_reply": "2025-04-04T13:53:19.744967Z"
    },
    "papermill": {
     "duration": 0.080755,
     "end_time": "2025-04-04T13:53:19.751783",
     "exception": false,
     "start_time": "2025-04-04T13:53:19.671028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc3\n",
    "from common_stuff import *\n",
    "start_inference(gpu=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T13:53:19.830117Z",
     "iopub.status.busy": "2025-04-04T13:53:19.829328Z",
     "iopub.status.idle": "2025-04-04T14:08:44.63478Z",
     "shell.execute_reply": "2025-04-04T14:08:44.634113Z"
    },
    "papermill": {
     "duration": 924.842711,
     "end_time": "2025-04-04T14:08:44.63648",
     "exception": false,
     "start_time": "2025-04-04T13:53:19.793769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "proc_exit_codes = await wait_for_subprocesses(\n",
    "    train_proc0, train_proc1, train_proc2, train_proc3,\n",
    "    infer_proc0, infer_proc1, infer_proc2, infer_proc3,\n",
    "    print_output=True or arc_test_set.is_fake\n",
    ")\n",
    "print(f'*** Subprocesses exit codes: {proc_exit_codes}')\n",
    "assert all(x==0 for x in proc_exit_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T14:08:44.811356Z",
     "iopub.status.busy": "2025-04-04T14:08:44.810792Z",
     "iopub.status.idle": "2025-04-04T14:08:45.692749Z",
     "shell.execute_reply": "2025-04-04T14:08:45.692117Z"
    },
    "papermill": {
     "duration": 0.969844,
     "end_time": "2025-04-04T14:08:45.694257",
     "exception": false,
     "start_time": "2025-04-04T14:08:44.724413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write submission\n",
    "from common_stuff import *\n",
    "with RemapCudaOOM():\n",
    "    model, formatter, dataset = None, MyFormatter(), None\n",
    "    decoder = Decoder(formatter, arc_test_set.split_multi_replies(), n_guesses=2, frac_score=True).from_store(infer_params['store'])\n",
    "    if use_aug_score or arc_test_set.is_fake: decoder.calc_augmented_scores(model=model, store=score_temp_storage, **aug_score_params)\n",
    "    submission = arc_test_set.get_submission(decoder.run_selection_algo(submission_select_algo))\n",
    "    with open('submission.json', 'w') as f: json.dump(submission, f)\n",
    "    if arc_test_set.is_fake:\n",
    "        decoder.benchmark_selection_algos(selection_algorithms)\n",
    "        with open('submission.json') as f: reload_submission = json.load(f)\n",
    "        print('*** Reload score:', arc_test_set.validate_submission(reload_submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T14:08:45.873159Z",
     "iopub.status.busy": "2025-04-04T14:08:45.872874Z",
     "iopub.status.idle": "2025-04-04T14:08:52.871766Z",
     "shell.execute_reply": "2025-04-04T14:08:52.871151Z"
    },
    "papermill": {
     "duration": 7.089823,
     "end_time": "2025-04-04T14:08:52.873753",
     "exception": false,
     "start_time": "2025-04-04T14:08:45.78393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization for inference results from submission.json\n",
    "if arc_test_set.is_fake:\n",
    "    from common_stuff import *\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib import colors\n",
    "    import json\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VISUALIZING RESULTS FROM SUBMISSION.JSON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check if submission file exists\n",
    "    submission_path = 'submission.json'\n",
    "    if not os.path.exists(submission_path):\n",
    "        print(f\"Submission file not found at {submission_path}\")\n",
    "    else:\n",
    "        print(f\"Found submission file: {submission_path}\")\n",
    "        \n",
    "        # Load submission data\n",
    "        with open(submission_path, 'r') as f:\n",
    "            submission_data = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded submission with {len(submission_data)} tasks\")\n",
    "        \n",
    "        # ARC color map\n",
    "        cmap = colors.ListedColormap(\n",
    "            ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
    "             '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "        norm = colors.Normalize(vmin=0, vmax=9)\n",
    "        \n",
    "        # Function to check if prediction is non-trivial (not just zeros)\n",
    "        def is_non_trivial_prediction(pred_array):\n",
    "            # Check if the prediction contains any non-zero values\n",
    "            return np.any(np.array(pred_array) > 0)\n",
    "        \n",
    "        # Function to visualize a single task result\n",
    "        def visualize_submission_result(task_id, task_data, submission_output, test_idx):\n",
    "            # Skip visualization if both predictions are just zeros\n",
    "            pred_1 = np.array(submission_output['attempt_1'])\n",
    "            pred_2 = np.array(submission_output['attempt_2'])\n",
    "            \n",
    "            if not is_non_trivial_prediction(pred_1) and not is_non_trivial_prediction(pred_2):\n",
    "                print(f\"  Skipping visualization for Task {task_id} - Test #{test_idx+1} (all predictions are zeros)\")\n",
    "                return False\n",
    "            \n",
    "            # Create visualization\n",
    "            fig = plt.figure(figsize=(15, 8))\n",
    "            grid_spec = plt.GridSpec(2, 3, width_ratios=[1, 1, 1])\n",
    "            \n",
    "            # Training examples (first one only for simplicity)\n",
    "            if task_data['train']:\n",
    "                # Train Input\n",
    "                ax1 = fig.add_subplot(grid_spec[0, 0])\n",
    "                ax1.imshow(task_data['train'][0]['input'], cmap=cmap, norm=norm)\n",
    "                ax1.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "                ax1.set_title(\"Training Input\")\n",
    "                ax1.set_xticks([])\n",
    "                ax1.set_yticks([])\n",
    "                \n",
    "                # Train Output\n",
    "                ax2 = fig.add_subplot(grid_spec[1, 0])\n",
    "                ax2.imshow(task_data['train'][0]['output'], cmap=cmap, norm=norm)\n",
    "                ax2.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "                ax2.set_title(\"Training Output\")\n",
    "                ax2.set_xticks([])\n",
    "                ax2.set_yticks([])\n",
    "            \n",
    "            # Test Input\n",
    "            if test_idx < len(task_data['test']):\n",
    "                ax3 = fig.add_subplot(grid_spec[0, 1])\n",
    "                ax3.imshow(task_data['test'][test_idx]['input'], cmap=cmap, norm=norm)\n",
    "                ax3.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "                ax3.set_title(f\"Test Input (Test #{test_idx+1})\")\n",
    "                ax3.set_xticks([])\n",
    "                ax3.set_yticks([])\n",
    "                \n",
    "                # Ground Truth (if available)\n",
    "                if 'output' in task_data['test'][test_idx]:\n",
    "                    ax4 = fig.add_subplot(grid_spec[1, 1])\n",
    "                    ax4.imshow(task_data['test'][test_idx]['output'], cmap=cmap, norm=norm)\n",
    "                    ax4.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "                    ax4.set_title(\"Ground Truth\")\n",
    "                    ax4.set_xticks([])\n",
    "                    ax4.set_yticks([])\n",
    "            \n",
    "            # Model Predictions\n",
    "            # Attempt 1\n",
    "            ax5 = fig.add_subplot(grid_spec[0, 2])\n",
    "            ax5.imshow(pred_1, cmap=cmap, norm=norm)\n",
    "            ax5.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "            ax5.set_title(\"Model Prediction (Attempt 1)\")\n",
    "            ax5.set_xticks([])\n",
    "            ax5.set_yticks([])\n",
    "            \n",
    "            # Attempt 2\n",
    "            ax6 = fig.add_subplot(grid_spec[1, 2])\n",
    "            ax6.imshow(pred_2, cmap=cmap, norm=norm)\n",
    "            ax6.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "            ax6.set_title(\"Model Prediction (Attempt 2)\")\n",
    "            ax6.set_xticks([])\n",
    "            ax6.set_yticks([])\n",
    "            \n",
    "            plt.suptitle(f\"Task {task_id} - Test Example #{test_idx+1}\", fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.9)\n",
    "            plt.show()\n",
    "            \n",
    "            # Calculate accuracy if ground truth is available\n",
    "            if 'output' in task_data['test'][test_idx]:\n",
    "                ground_truth = np.array(task_data['test'][test_idx]['output'])\n",
    "                \n",
    "                # Check accuracy of both attempts\n",
    "                results = []\n",
    "                match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n",
    "                results.append(f\"Attempt 1: {'✓' if match_1 else '✗'}{' (zeros)' if not is_non_trivial_prediction(pred_1) else ''}\")\n",
    "                \n",
    "                match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n",
    "                results.append(f\"Attempt 2: {'✓' if match_2 else '✗'}{' (zeros)' if not is_non_trivial_prediction(pred_2) else ''}\")\n",
    "                \n",
    "                print(f\"  Results: {', '.join(results)}\")\n",
    "                \n",
    "                # Display task statistics\n",
    "                print(f\"  Shape - Ground Truth: {ground_truth.shape}, Prediction 1: {pred_1.shape}, Prediction 2: {pred_2.shape}\")\n",
    "                print(f\"  Values - Ground Truth unique values: {np.unique(ground_truth)}\")\n",
    "                print(f\"          Prediction 1 unique values: {np.unique(pred_1)}\")\n",
    "                print(f\"          Prediction 2 unique values: {np.unique(pred_2)}\")\n",
    "            print()\n",
    "            return True\n",
    "        \n",
    "        # Process ALL results from submission (no limit)\n",
    "        visualized_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        # Get a list of tasks in the submission\n",
    "        task_ids = list(submission_data.keys())\n",
    "        \n",
    "        # Collect all task/test combinations\n",
    "        all_predictions = []\n",
    "        for task_id in task_ids:\n",
    "            if task_id in arc_test_set.queries:\n",
    "                task_data = arc_test_set.queries[task_id]\n",
    "                for test_idx, test_prediction in enumerate(submission_data[task_id]):\n",
    "                    # Check if we have ground truth available\n",
    "                    has_ground_truth = (task_id in arc_test_set.replies and \n",
    "                                        test_idx < len(arc_test_set.replies[task_id]))\n",
    "                    \n",
    "                    # Check if predictions are non-trivial\n",
    "                    pred_1 = np.array(test_prediction['attempt_1'])\n",
    "                    pred_2 = np.array(test_prediction['attempt_2'])\n",
    "                    has_non_zero_pred = is_non_trivial_prediction(pred_1) or is_non_trivial_prediction(pred_2)\n",
    "                    \n",
    "                    # Score based on correctness if ground truth is available\n",
    "                    score = 0\n",
    "                    if has_ground_truth and has_non_zero_pred:\n",
    "                        ground_truth = np.array(arc_test_set.replies[task_id][test_idx])\n",
    "                        \n",
    "                        match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n",
    "                        match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n",
    "                        score = match_1 + match_2\n",
    "                        \n",
    "                    all_predictions.append((task_id, test_idx, score, has_ground_truth, has_non_zero_pred))\n",
    "        \n",
    "        # Sort by whether they have ground truth first, then by score\n",
    "        all_predictions.sort(key=lambda x: (-int(x[3]), -x[2]))\n",
    "        \n",
    "        # Print summary before visualization\n",
    "        print(f\"\\nFound {len(all_predictions)} total predictions to visualize\")\n",
    "        \n",
    "        # Visualize all tasks\n",
    "        for task_id, test_idx, score, has_ground_truth, has_non_zero_pred in all_predictions:\n",
    "            # Get task data and predictions\n",
    "            task_data = arc_test_set.queries[task_id]\n",
    "            submission_output = submission_data[task_id][test_idx]\n",
    "            \n",
    "            # Visualize this task\n",
    "            score_info = f\" (Score: {score}/2)\" if has_ground_truth and has_non_zero_pred else \" (no ground truth)\" if not has_ground_truth else \" (all zeros - no score)\"\n",
    "            print(f\"\\nTask: {task_id} - Test #{test_idx+1}{score_info}\")\n",
    "            \n",
    "            # Only increment visualized_count if actually visualized\n",
    "            if visualize_submission_result(task_id, task_data, submission_output, test_idx):\n",
    "                visualized_count += 1\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        \n",
    "        print(f\"\\nVisualized {visualized_count} inference results (skipped {skipped_count} with all-zero predictions)\")\n",
    "        \n",
    "        # Calculate overall accuracy statistics\n",
    "        if arc_test_set.is_fake:\n",
    "            total_tests = 0\n",
    "            total_scored_tests = 0\n",
    "            correct_attempt1 = 0\n",
    "            correct_attempt2 = 0\n",
    "            correct_any = 0\n",
    "            zero_predictions = 0\n",
    "            \n",
    "            for task_id, test_predictions in submission_data.items():\n",
    "                if task_id in arc_test_set.replies:\n",
    "                    for test_idx, test_prediction in enumerate(test_predictions):\n",
    "                        if test_idx < len(arc_test_set.replies[task_id]):\n",
    "                            total_tests += 1\n",
    "                            \n",
    "                            ground_truth = np.array(arc_test_set.replies[task_id][test_idx])\n",
    "                            pred_1 = np.array(test_prediction['attempt_1'])\n",
    "                            pred_2 = np.array(test_prediction['attempt_2'])\n",
    "                            \n",
    "                            # Check if both predictions are all zeros\n",
    "                            if not is_non_trivial_prediction(pred_1) and not is_non_trivial_prediction(pred_2):\n",
    "                                zero_predictions += 1\n",
    "                                continue\n",
    "                            \n",
    "                            # Only count tests with at least one non-zero prediction\n",
    "                            total_scored_tests += 1\n",
    "                            \n",
    "                            match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n",
    "                            match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n",
    "                            \n",
    "                            if match_1: correct_attempt1 += 1\n",
    "                            if match_2: correct_attempt2 += 1\n",
    "                            if match_1 or match_2: correct_any += 1\n",
    "            \n",
    "            if total_tests > 0:\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"OVERALL ACCURACY STATISTICS\")\n",
    "                print(\"=\"*80)\n",
    "                print(f\"Total test examples: {total_tests}\")\n",
    "                print(f\"Test examples with zero predictions (excluded from accuracy): {zero_predictions}\")\n",
    "                print(f\"Test examples included in accuracy calculation: {total_scored_tests}\")\n",
    "                \n",
    "                if total_scored_tests > 0:\n",
    "                    print(f\"Correct on attempt 1: {correct_attempt1}/{total_scored_tests} ({correct_attempt1/total_scored_tests:.2%})\")\n",
    "                    print(f\"Correct on attempt 2: {correct_attempt2}/{total_scored_tests} ({correct_attempt2/total_scored_tests:.2%})\")\n",
    "                    print(f\"Correct on either attempt: {correct_any}/{total_scored_tests} ({correct_any/total_scored_tests:.2%})\")\n",
    "                else:\n",
    "                    print(\"No non-zero predictions to calculate accuracy\")\n",
    "                    \n",
    "                print(f\"Overall completion rate: {total_scored_tests/total_tests:.2%} of tests have non-zero predictions\")\n",
    "                print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping inference visualization - not in fake test mode\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11483707,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 5793177,
     "sourceId": 9515958,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 157175,
     "modelInstanceId": 134422,
     "sourceId": 158171,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1059.350941,
   "end_time": "2025-04-04T14:08:54.10074",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-04T13:51:14.749799",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
